---
title: "Predicting Tenant Satisfaction with Slovakian Apartments"
subtitle: "Final Report"
author: "Meera Trivedi"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  warning: false
  echo: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon=false}

## Github Repo Link

[My GitHub Repo](https://github.com/stat301-2-2024-winter/final-project-2-meeratrivedi.git)

:::

## Introduction

#### Data Source
My dataset is called *Real Estate Dataset* and has been downloaded from Kaggle, on which it is described as a dataset that explores “spatial dynamics, price, and quality of apartments in Slovakia”. The author himself obtained the data from scraping advertising data from the Nehnutel'nosti website as of November 2023, so this is fairly recent data, and only pertains to apartment listings. The Nehnutelnosti website has real estate listings and provides services to help find available properties in Slovakia. Each observation refers to an apartment listing and the variables measured about each listing range from costs, to dimensions, to features, to the safety and environment around the apartment’s building. It was posted to Kaggle by Arnold Kakas, who notes that he has intentionally left the dataset completely raw for the purpose of people using it to learn data cleaning, data visualization, and training predictive models, which is the purpose of this project. 

#### Prediction Problem
My prediction problem was going to be to use various types of data on apartments in Slovakia, ranging from data on the actual construction of the building to the environment around it, to predict the overall quality of living of the people living in those properties. At first, quality of living was a numerical variable corresponding to an index but because of this I could change it to a factor variable with different ordered levels to make it a classification problem. However, since there were still many levels I decided to calculate a new target variable: satisfaction. It has two levels, “satisfied” and “unsatisfied”. I will explain how I calculated it further on. I think it’s very useful to uncover relationships between the satisfaction and every available aspect of a building because in order for a building to be marketable people have to gain some benefit from living there. The data I found happens to be from Slovakia which is interesting because I don't ever see that country represented in the media so it would be nice to learn a little more about it. The main reason I chose this data though is because I am a sophomore moving off campus next year so I have looked into apartments both by myself and with friends and finally signed my own lease. In the process I had to evaluate data on each building to decide what would be the best option for me. I also used to watch house flipping and renovating shows with my dad when I was younger all the time so I thought it might be fun to look into what would make a property successful, and a way to measure that is satisfaction. A model to predict something as useful as satisfaction would be helpful to understand the value of a building based on its various metrics and categorical data.

## Data Overview
```{r}
#| message: false
# load packages ----
library(tidyverse)
library(here)
library(skimr)
library(naniar)
library(janitor)
library(gt)
library(tidymodels)

#load data
load(here("data/clean_qol.rda"))
load(here("data/estate_split.rda"))
```

#### Original Data
This dataset has 27 variables and 15,403 observations. Out of the 27 variables, 7 are character variables: the name of the apartment, whether it is in its original condition or has gone through some sort of renovation, what energy certificate the building has, the material the apartment is made out of, like brick or paneling, the geographical direction it faces, the type of apartment it is in terms of the number of bedrooms (ex: 3-bedroom) and the name of the district in Slovakia it is in. The other 20 are numerical, and consist of the price of the listing in euros, a number on an Index of Living from 0-10; according to Kaggle this was “calculated by the Slovak startup City Performer” using the values of these 6 variables in the dataset corresponding to environment, quality of living, safety, transportation, services, and relaxation. Each value of these 6 variables represents an index rating for just that variable. Other variables include the area in square meters of the apartment, the energy costs in euros for that apartment, a binary variable with a value of 1 if the provision of the agency is included in the price, and if not is 0, the year it was built, the most recent year any sort of construction work was done on it, the floor of the building the apartment is on, whether or not the building has a lift or a cellar and more features, where the value of each is the number of features each individual apartment or building has.

#### Cleaned Data
The cleaned dataset has 28 variables and 4132 observations. All observations with missing values for the quality of living index variable were filtered out and the satisfaction variable was added. This is the dataset used for data splitting.

#### Missingness
Missingness is explored here using @fig-plot1. The variables that have missingness have missingness for almost all of their values so many of them may not be useful predictor variables.

![Missingness](setup/figures/missingness_whole.png){#fig-plot1}


#### Target Variable Analysis

@fig-bar1 is a bar plot that shows the count of each level of satisfaction. This variable was calculated using the quality of living variable. The value of this variable is “unsatisfied” if the quality of living is below 85 (out of 100) and “satisfied” if the quality of living is from 85 to 100 (out of 100). Since the quality of living variable is on a scale out of 100 I decided to use the US grading system, which is also out of 100, to determine what would be a good cutoff for satisfaction versus dissatisfaction. “C” range grades are typically considered average and “B” range grades are typically considered above average, and “A” is of course the highest achievable grade. So since 85 is the average of the “B” range grades I thought it made sense as a differentiator between above average and not. 

![Slovakian Apartment Tenants Satisfaction](setup/figures/target_analysis.png){#fig-bar1}

The target variable was originally intended to be the quality of living as a factor but since there was quite a bimodal distribution with a much higher density in values above 60 out of 100, it made more sense to use a factor with two levels to counteract what would have been a massive class imbalance. Using the US grading system helped to balance the distribution to an extent but there are still noticeably more values for satisfied than unsatisfied. However, considering the context of this data this makes sense, we would hope most people are happy with their living situation. See the Appendix for the distribution of the quality of living variable. 

## Methods 

#### Data Splitting
The cleaned data was split using stratified splitting by the target variable: satisfaction, and using an 80%/20% split because the dataset is fairly large. This is a common split to use with large datasets. This means 80% of the data goes into the training dataset and the other 20% goes into the testing dataset. 

#### Resampling
The data was resampled using V-fold cross validation with 10 folds and 5 repeats which will then result in 50 models per fit. I used this method of resampling instead of bootstrap resampling because that would introduce the possibility of some observations in a bootstrap sample appearing multiple times with others not appearing at all. V-fold cross validation also has lower bias, and is more commonly used than bootstrap for big datasets. I used 10 folds and 5 repeats because multiples of 5 are generally common benchmark numbers and my computer could handle this much resampling.

#### Recipes
Six recipes were used to create the models. Unfortunately multiple of my predictor variables had many missing values. They must have been added to the dataset as variables because a few apartments had data for them, but if most apartments don’t have data points for those variables, they can’t be useful predictors in a prediction model. As an example, one variable is about the type of energy certificate the apartment has but the most frequent value was “none” other than a missing value meaning that apartment didn’t have an energy certificate anyway. This means having an energy certificate can’t be that useful of a predictor if the majority of apartments either don’t have a certificate or have a missing value for it. 

For simplicity, the basic recipes have all the variables with missing data removed, as well as the factor variables with many levels removed to avoid overfitting and overworking the computer. It imputes the area variable because there is only a small amount of missingness, so imputation would only be making up for a small percentage of the values. And it creates dummy variables for the nominal predictors, removes variables with zero variance, and centers and scales all predictors. These are standard steps to take for a basic recipe.

The main recipe does all of this as well except it does not remove all of the variables with missingness, instead for two variables: the total floors the building has and the floor number the apartment is on, it mutates them to be factors with three levels; one level replaces missing values with the word “unknown”, since seeing something is unknown could be interesting instead of removing those observations entirely. These differences will help evaluate if knowing something is unknown is actually useful in making predictions. These two variables were chosen specifically to be mutated because they have about half of their values missing so this unknown level would have a lot of values but not so much that the variables should be considered totally unusable in all recipes.

The “interact” recipe builds on this recipe and additionally uses interaction terms to analyze the impact of them separately. EDA was conducted to understand the relationships between predictor variables to determine interaction terms. First I explored price (on a log 10 scale for visual purposes) by satisfaction by different levels of variables that represented rankings on different aspects of the apartment to see if there were differences in the satisfaction for each range. Every variable that represented rankings were factored and split into 3 equal levels (so if available ranking values were from 4-99 each level was ⅓ of that range). There were considered to be clear differences in satisfaction if the level that had the most density for each range was different for the different ranges. This would mean that the different ranges could help contribute to determining satisfaction and so it would make sense to create an interaction term. This process was used to justify creating interaction terms for price and environment, price and relaxation ability, price and available services, and price and transportation services. Next, since I thought safety and environment could be related I explored their relationship by the two levels of satisfaction in the form of a faceted boxplot with environment factored to have 5 equal levels representing ranges of values and found that the distributions were quite different for the two levels of satisfaction especially between the middle two ranges of environment so it made sense to create an interaction term between safety and environment as well. Finally I explored the relationship between price and area since typically the bigger an apartment is the more expensive it is but found that the relationship did not differ much by satisfaction so I decided not to create an interaction term for this. So to conclude, I created 5 interaction terms based on this EDA, and the graphs can be found in the appendix. 

The tree versions of the basic, main, and interact recipes do the same things as the aforementioned basic, main, and interact recipes respectively; however they use one-hot encoding.

#### Model Types
Since this is a classification problem, the models I will be creating will be: null, logistic, elastic net, nearest neighbors, random forest, and boosted tree. The null model only uses the basic recipe and the other models all use all three recipe types, so there are three different models of each type corresponding to each recipe type. The null, logistic, and elastic net models use parametric recipes and the nearest neighbors, random forest, and boosted tree use tree recipes. 

#### Tuning Parameters
The elastic net, nearest neighbors, random forest, and boosted tree models all use tuning.

The elastic net model uses tuning for the mixture and penalty. The tuning ranges were updated so that the range of mixture is from 0 to 1. The lower bound is 0 because that would indicate a fully ridge model and the upper bound is 1 because that would indicate a fully lasso model, so the range for a combination of the two model types should be in between. The range of penalty is from -5 to 0.2. These numbers are on log10 transformation so the real bounds would be 10^-5 to 10^0.2. This becomes important when looking at the optimal values of the hyperparameters later on. I updated these values to shift the range higher than the default to test values that would have more impact than if they were more centered around 0. 

The nearest neighbor model uses tuning for the neighbors variable but it is my understanding that the default tuning range works well so there was no need to update it.

The random forest model uses tuning for the minimal node size and number of sample predictors. The minimal node size is the minimum number of data points required for the node to be split further. Using tuning for these hyperparameters is helpful to improve the accuracy of these models. Since the default range of minimal node size works well there was no need to update it but the range of the number of sample predictors was updated to be from 1 to 11 because we want the upper bound to be the maximum number of predictor variables that could be randomly drawn so the range includes all the predictor variables. Tuning was not used for the trees variable since we would want to use a fixed value: as high of a number as a computer could handle. Since 500 is the default and 1000 would be a lot, I used 750 since it is a benchmark value and the average between the two. 

The boosted tree model uses tuning for number of sample predictors, minimal node size, and learning rate, which is a rate that the weak learners' predictions get scaled. The range of number of sample predictors was updated in the same way as it was for the random forest model and the range of the minimal node size was not updated again. The range of learning rate was updated to (-5, -0.2) since the upper bound should be close to but less than 0 and lower ranges of learning rates are better when more trees are used to achieve the same level of fit. These numbers are including on log10 transformation so the real bounds would be 10^-5 to 10^-0.2 which again becomes important when looking at optimal values of hyperparameters later on. 


#### Assessment Metrics
Since this is a classification problem, the assessment metric will be the area under the ROC curve. The ROC curve shows the relationship between the true positive rate: sensitivity, and false positive rate: 1 - specificity, for different thresholds of classification. It is a good measure to use for binary classification, in this case between satisfied and unsatisfied. The area under the curve can be from 0 to 1, with a higher measure indicating better performance. This is a good metric to use when there is imbalance in a dataset between levels of the target variable and since there seem to be noticeably more people satisfied with their apartments than not, this could be considered an imbalance.  

## Models

#### Basic Recipes

First we can explore the 6 different model types fitted to the basic recipes. @tbl-basic shows that in comparison to the null model all models have a higher measure of performance, meaning making complex models is useful in making more accurate predictions. In this case, the random forest model performs the best with a performance measure of 0.958, which is very close to 1. It also has the lowest standard error out of all the models except the null. Standard error is not the primary assessment metric here but this information is still interesting nonetheless. 
```{r}
#| label: tbl-basic
#| tbl-cap: Evaluation of six models on the basic recipes. 

load(here("basic analysis/basic results/metrics.rda"))

metrics |> 
  select(mean, n, std_err, model) |> 
  relocate(model) |>
  arrange(desc(mean)) |> 
  gt() |> 
  tab_header(title = md("**Assessment Metrics**"), 
             subtitle = "All Models - Basic Recipe") |>
  cols_label(std_err = md("Standard Error"), 
      mean = md("Mean ROC AUC"), 
      n = md("Number of Models"), 
      model = md("Model Type")) |> 
  fmt_number(
    columns = mean, 
    decimals = 3) |> 
  fmt_number(
    columns = std_err, 
    decimals = 5) 

```

#### Main Recipes

Next we can explore the 5 model types other than the null model on the main recipe. @tbl-main shows that the random forest model still performs better than the other models by having a much higher measure of performance of 0.956, which is very close to 1. It still has the lowest standard error out of all the models. The order of the models by best performance is the same as on the basic recipes. 
```{r}
#| label: tbl-main
#| tbl-cap: Evaluation of six models on the main recipes. 
load(here("main analysis/main results/metrics2.rda"))

metrics2 |> 
  select(mean, n, std_err, model) |> 
  arrange(desc(mean)) |> 
  relocate(model) |> 
  gt() |> 
  tab_header(title = md("**Assessment Metrics**"), 
             subtitle = "All Models - Main Recipe") |>
  cols_label(std_err = md("Standard Error"), 
             mean = md("Mean ROC AUC"), 
             n = md("Number of Models"), 
             model = md("Model Type")) |> 
  fmt_number(
    columns = mean, 
    decimals = 3) |> 
  fmt_number(
    columns = std_err, 
    decimals = 7)
```

#### Interaction Recipes
Finally we can explore the 5 models other than the null on the recipe that additionally includes interaction terms. @tbl-interact shows that the random forest model still performs better than the other models by having a much higher measure of performance of 0.954, which is very close to 1. It still has the lowest standard error out of all the models. The order of the models by best performance is the same as on the basic and main recipes. 
```{r}
#| label: tbl-interact
#| tbl-cap: Evaluation of six models on the interaction recipes. 

load(here("interact analysis/interact results/metrics3.rda"))

metrics3 |> 
  select(mean, n, std_err, model) |> 
  arrange(desc(mean)) |> 
  relocate(model) |> 
  gt() |> 
  tab_header(title = md("**Assessment Metrics**"), 
             subtitle = "All Models - Interaction Recipe") |>
  cols_label(std_err = md("Standard Error"), 
             mean = md("Mean ROC AUC"), 
             n = md("Number of Models"), 
             model = md("Model Type")) |> 
  fmt_number(
    columns = mean, 
    decimals = 3) |> 
  fmt_number(
    columns = std_err, 
    decimals = 7) 
```

#### Best Hyperparameters

This section will explore the best hyperparameters for each model type on each recipe to understand what values of the tuned parameters were the most beneficial to predicting. 
```{r}
load(here("final analysis/best hyperparameters/bestparams_en.rda"))
load(here("final analysis/best hyperparameters/bestparams_bt.rda"))
load(here("final analysis/best hyperparameters/bestparams_rf.rda"))
load(here("final analysis/best hyperparameters/bestparams_knn.rda"))
```
##### Random Forest & Boosted Tree
@tbl-rf and @tbl-bt show the best hyperparameter values for the random forest and boosted tree models respectively. It was definitely a good choice to set the range of number of sample predictors to include the maximum number of predictor variables because that value ended up being the best hyperparameter for the feature engineered recipes for both the random forest and boosted tree models. A good learning rate with a high number of trees should be close to 0 so it makes sense that the best value is close to 0. It would be interesting to test an even lower range to see if that would be more helpful. 
```{r}
#| label: tbl-rf
#| tbl-cap: Best hyperparameters for Random Forest models on all three tree recipes.
 
bestparams_rf |> 
  gt()|> 
  tab_header(title = md("**Best Hyperparameters - Random Forest**"), 
             subtitle = "Random Forest - All Recipes") |>
  cols_label(recipe = md("Recipe"), 
             mtry = md("Number of <br> Sample Predictors"), 
             min_n = md("Minimal <br> Node Size")) 
```

```{r}
#| label: tbl-bt
#| tbl-cap: Best hyperparameters for Boosted Tree models on all three tree recipes.
bestparams_bt |> 
  gt()|> 
  tab_header(title = md("**Best Hyperparameters - Boosted Tree**"), 
             subtitle = "Boosted Tree - All Recipes") |>
  cols_label(recipe = md("Recipe"), 
             mtry = md("Number of <br> Sample Predictors"), 
             min_n = md("Minimal<br> Node Size"), 
             learn_rate = md("Learning <br> Rate")) |> 
  fmt_number(
    columns = learn_rate, 
    decimals = 3)
```


##### Nearest Neighbors
@tbl-knn shows the best hyperparameter values for the nearest neighbors model. A higher number of neighbors leads to stronger predictions because more neighbors handle outliers better; however a very high number of neighbors computational efficiency can decrease so it makes sense that the best values of neighbors for making predictions are numbers much greater than 1 but not very high numbers.
```{r}
#| label: tbl-knn
#| tbl-cap: Best hyperparameters for Nearest Neighbors models on all three tree recipes.

bestparams_knn |> 
  gt()|> 
  tab_header(title = md("**Best Hyperparameters - Nearest Neighbors**"), 
             subtitle = "Nearest Neighbors - All Recipes") |>
  cols_label(recipe = md("Recipe"), 
             neighbors = md("Neighbors")) 
```
##### Elastic Net

@tbl-en shows the best hyperparameter values for the elastic net model. It was really interesting that the best hyperparameter value for the mixture was consistently 0, meaning that a full ridge regression type of model indicated higher performance. It could be interesting in the future to split this up and explore ridge and lasso regression separately to see if this makes differences in performance. The optimal value of penalty is on the higher end of the range from 10^-5 to 10^0.2. The optimal value being on the higher end of the range means that the model likely benefits from stronger regularization which controls overfitting and improves generalizability. It might be interesting to test a range shifted a bit higher than this one in the future to see if that increases performance. 

```{r}
#| label: tbl-en
#| tbl-cap: Best hyperparameters for Elastic Net models on all three non-tree recipes.

bestparams_en |> 
  gt()|> 
  tab_header(title = md("**Best Hyperparameters - Elastic Net**"), 
             subtitle = "Elastic Net - All Recipes") |>
  cols_label(recipe = md("Recipe"), 
             penalty = md("Penalty"), 
             mixture = md("Mixture")) |> 
  fmt_number(
    columns = penalty, 
    decimals = 3)
```


#### Compare & Contrast

Now that all the models have been built on various recipes, we can compare their performances by recipe. @tbl-comparison shows that overall, the random forest model performs the best regardless of the recipe but that it performs better on the basic recipe than the ones that include more feature engineering steps. The same is true for the second-best performing model: the boosted tree. There is seemingly no difference that the interaction terms have made because the measure of performance is the same for both more feature engineered recipes and the performance on the basic recipe is higher. On the next best performing model: nearest neighbors, the interaction terms do seem to have lead to a considerable increase in performance but the basic recipe still performs better. The two lowest performing models other than the null: elastic net and logistic regression both had the more feature engineered recipes perform better than the basic recipe but by a very small difference. And the recipe with interaction terms performs almost the same as the recipe without interaction terms for both model types. This has led me to conclude that overall, the interaction terms created did not help prediction and that handling missingness by keeping certain variables and using the fact that the value is unknown to help predict, was actually not useful in this case. More of the time, we can get a higher performing model by removing these variables with missingness instead of allowing that information to influence our predictions. It was an interesting idea to explore, if missingness could be used to our advantage, but in this case it seems to be more useful to simply remove it. Overall, the model with the highest measure of the area under the ROC curve is the random forest model on the basic recipe, and so that model performs the best and is selected as the final winning model. It is not surprising that this model performed the best since using tuning and a fixed high number of trees is designed to improve accuracy. 

```{r}
#| label: tbl-comparison
#| tbl-cap: Comparison of All Model Types

metrics |> 
  mutate(recipe = "basic") |> 
  bind_rows(metrics2 |> mutate(recipe = "main")) |>
  bind_rows(metrics3 |> mutate(recipe = "interactions")) |> 
  select(mean, n, std_err, model, recipe) |> 
  relocate(recipe, model) |>
  group_by(model) |> 
  arrange(desc(mean)) |> 
  gt() |> 
  tab_header(title = md("**Assessment Metrics**"), 
             subtitle = "All Models - All Recipes") |>
  cols_label(recipe = md("Recipe"), 
    std_err = md("Standard Error"), 
    mean = md("Mean ROC AUC"), 
    n = md("Number of Models")) |> 
  fmt_number(
    columns = mean, 
    decimals = 3) |> 
  fmt_number(
    columns = std_err, 
    decimals = 5) |> 
  row_group_order(groups = c("rf", "bt", "knn", "en", "logistic")) |> 
  tab_options(row_group.background.color = "grey50")

```

## Final Model Analysis
This section will assess the best model: the random forest on the basic recipe, in more detail. 

#### ROC AUC & Accuracy
@tbl-roc_accuracy shows two performance measures for the winning model, the primary one: area under the ROC curve and something more intuitive, accuracy. Like the area under the ROC curve we want the accuracy to be closer to 1 than 0 since 1 would mean 100% accuracy and we'd like to get as close to that as possible. The fact that the accuracy is 0.917 is quite impressive, and means that this model performs very well. The same logic is used to assess the area under the curve and its high value of 0.969 is quite impressive as well.

```{r}
#| label: tbl-roc_accuracy
#| tbl-cap: Assessment Metrics for the Winning Model

load(here("final analysis/results/accuracy_roc_auc.rda"))

#accuracy & roc_auc
accuracy_roc_auc |> 
  select(-.estimator) |> 
  gt() |> 
  tab_header(title = md("**ROC AUC & Accuracy**")) |>
  cols_label(.metric = md("Assessment Metric"), 
             .estimate = md("Estimate")) |>
  fmt_number(
    columns = .estimate, 
    decimals = 3)
```

#### ROC Curve
Next, @fig-plot2 shows a visual representation of the ROC curve to better understand our assessment metric of the area under its curve. Since we want the area to be as big as it can be, we want the curve to be as close to almost outlining the top left corner of the graph as it can be. This is the case here, as it is clear that this ROC curve takes on that desired shape and has a very large area between its border and the dotted line. This would mean a high true positive rate and a low false positive rate which means minimal error in prediction. 

![ROC Curve for the Winning Model](final analysis/results/roc_curve.png){#fig-plot2}


#### Confusion Matrix 
To further explore this model we can use the confusion matrix in @fig-bar2 to compare predictions to the accurate values. The top left is the number of correctly predicted satisfied apartment owners, 475, the bottom right is the number of correctly predicted unsatisfied apartment owners, 283. The bottom left is the number of wrongly predicted satisfied apartment owners as unsatisfied, only 12, and the top right is the number of wrongly predicted unsatisfied apartment owners as satisfied, 57. I am surprised that many more people who are unsatisfied with their apartment were incorrectly predicted than those who were really satisfied and wrongly predicted. This could have something to do with the original class imbalance. 

![Confusion Matrix for the Winning Model](final analysis/results/conf_mat.png){#fig-bar2}

Overall, this model is clearly pretty successful in its predictions as primarily shown by our primary assessment metric of the area under the ROC curve. It is performing much better than any of the other models so it can be concluded that it is worth it to build a more complex predictive model. 

## Conclusion

In conclusion, there are a lot of different ways to build predictive models and a lot of different ways to go about feature engineering. On Kaggle, the author of this dataset posted a comment saying that this dataset is designed to be used to learn how to complete EDA and prediction models, which was the purpose of this project so it made sense that there was a lot of cleaning to be done. I began by using the quality of living as a target variable but the distribution was very bimodal with the highest density of values above 60 out of 100. So to combat this imbalance of data I used the values of this variable to calculate the satisfaction variable. It is now much easier to just classify someone as satisfied or unsatisfied and there is much less of an imbalance between the classes. Since I could only use observations that did not have missing values for my target variable, all observations with missing values for the quality of living variable needed to be filtered out, and unfortunately that cut down the dataset to only 4132 observations. Much of the raw dataset was not applicable to this prediction problem and even within this cleaned dataset, missingness was prevalent in a lot of variables. Many had over half of their values missing so it made sense to remove them, but for variables with around half or less than half I tried to combat it in a different way by utilizing missing values to our advantage and giving them their own unknown level. Perhaps keeping those variables and using the fact that values were unknown as another piece of information could have helped prediction, but in this case it didn’t make much of a difference. I was quite surprised because here in Evanston people tend to place importance in the size of their building and what floor they are on. I live on the top floor of my building and I would say that contributes to my satisfaction, but I guess in this case it was not as important in predicting satisfaction. I also have friends who live off campus and place importance in the height of their building as well, so I really thought these would be important predictors, but I guess including them to the best ability possible did not make much of a difference in performance. I am also surprised that creating interaction terms did not help performance much. I thought based on my EDA that some of the different correlations between variables could make a difference in performance but I guess not doing so helped prediction more in most cases. The main thing I learned from this is that part of feature engineering is learning when certain predictors are not helpful and that removing them sometimes is the best option to improve performance and model accuracy. I also learned that tuning models helps find the best values of hyperparameters to maximize model performance so it makes sense that those models performed better than the two models that were not tuned. Finally, I learned over the course of this quarter and through this project that random forest models often do well, even the relative best a lot of times! This is probably because setting a high number of trees improves accuracy in addition to tuning other hyperparameters. Something I would potentially want to explore in the future would be combining more of the factor variables into fewer levels that include ranges of factors. Including factor variables with too many levels can lead to overfitting which is partly why they were removed, a lot of them also had missingness, but throughout this project I learned how to combine levels into ranges so that might be an interesting thing to explore in a new recipe in the future. However since a lot of these factors had more missingness than the variables I mutated for this project that probably wouldn’t help too much but it would be worth a try! I am also curious to learn more about the different index variables. Exploring them for the EDA was really interesting and perhaps for a project more like last quarter’s I would be curious to explore all the relationships between variables. But that wouldn’t be relevant to the purpose of this project. I would also want to see some of the relationships between variables I had to remove from my recipe and my target variable out of curiosity. 

## References

Kakas, Arnold. (2023, Nov). *Real Estate Dataset*. Kaggle. [https://www.kaggle.com/datasets/arnoldkakas/real-estate-dataset/data](https://www.kaggle.com/datasets/arnoldkakas/real-estate-dataset/data)

Nehnutel'nosti. (2023). *Nehnutel'nosti*. [https://www.nehnutelnosti.sk/](https://www.nehnutelnosti.sk/)

## Appendix - EDA 

Shown here is the EDA used to justify the creation of interaction terms. Even though they did not end up being as useful as I initially believed, there were enough noticeable differences in relationships between certain variables that exploring these interaction terms was an interesting idea. 

@fig-plot3 shows the relationship between price and satisfaction for three different ranges of environmental rankings, which represent tenants ranking their environment on a scale of 0-100 with higher numbers meaning higher happiness. As you can see, for the first third and last third of the rankings, more people are satisfied, but for the middle third more people are unsatisfied. This means the relationship between price and satisfaction differs by environment and that is why an interaction term was created. I used this same logic to evaluate @fig-plot4, @fig-plot5, and @fig-plot6. 

![Price by Satisfaction by Environment](setup/figures/price_environment.png){#fig-plot3}

![Price by Satisfaction by Relaxation](setup/figures/price_relax.png){#fig-plot4}

![Price by Satisfaction by Services](setup/figures/price_services.png){#fig-plot5}

![Services by Satisfaction by Transportation](setup/figures/services_transport.png){#fig-plot6}

I used similar logic to evaluate @fig-plot7 since the distributions of safety by different ranges of environmental rankings are different for each level of satisfaction. (I used 5 equal ranges this time instead of three to see more specific smaller range levels).


![Safety by Environment by Satisfaction](setup/figures/safety_environment.png){#fig-plot7}

Finally, I decided not to create an interaction term for price and area even though in theory the fact that they might interact made sense to me. When explored in the faceted barplot @fig-plot8 the range of values of area went to over 4500 square meters but very few apartments were bigger than 1000 square meters so there may not have been enough really big apartments to see if that made a considerable difference in satisfaction, at least enough to create an interaction term. 

![Price by Satisfaction by Area Ranges](setup/figures/price_area.png){#fig-plot8}

The density plot in @fig-plot9 shows the bimodal distribution of the quality of living variable in case one was curious and wanted to see what that looked like on the raw dataset. 

![Quality of Living Distribution](setup/figures/quality_of_living.png){#fig-plot9}